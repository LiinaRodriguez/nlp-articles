{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aNIiSXNJmYx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aNIiSXNJmYx",
        "outputId": "541382c4-a1d7-46c0-bba7-126b8834332e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install --upgrade openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2887714f",
      "metadata": {
        "id": "2887714f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import spacy\n",
        "import unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from langdetect import detect\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0xBKNW7JXwgA",
      "metadata": {
        "id": "0xBKNW7JXwgA"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00a31c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f00a31c5",
        "outputId": "53b77fe2-6780-4422-baf7-651da8ed8640"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "# Variables globales para parámetros\n",
        "n_topics = 5\n",
        "n_iter = 2500\n",
        "learning_decay = 0.6\n",
        "max_features = 5000\n",
        "max_df = 0.75\n",
        "min_df = 2\n",
        "n_top_words = 100\n",
        "\n",
        "# Cargar modelos de spaCy\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "nlp_en.max_length = 5000000\n",
        "nlp_de = spacy.load('de_core_news_sm')\n",
        "nlp_de.max_length = 5000000\n",
        "\n",
        "# Stopwords\n",
        "stopwords_en = set(stopwords.words('english'))\n",
        "stopwords_de = set(stopwords.words('german'))\n",
        "\n",
        "# Stopwords extendidas\n",
        "extra_stopwords = {\n",
        "    'al', 'biomarker', 'fluorescent', 'canis', 'keywords', 'conclusion', 'dogmammary', 'method',\n",
        "    'lupus', 'carotid', 'subject', 'paper', 'study', 'neoplasia', 'hdmi', 'granuloma', 'august',\n",
        "    'carcinoma', 'authors', 'discussion', 'microscopy', 'fluorescence', 'bratislava', 'slicing',\n",
        "    'asthma', 'ptx', 'pathologist', 'myocardium', 'slide', 'transfection', 'malignancy', 'https',\n",
        "    'immunohistochemistry', 'benign', 'metastasis', 'pathology', 'gmbh', 'harvest', 'phosphatase',\n",
        "    'stage', 'clinics', 'centrifugation', 'department', 'publication', 'january', 'model', 'biomedicine',\n",
        "    'email', 'procedure', 'receptor', 'microvascular', 'oncology', 'abstract', 'result', 'gct',\n",
        "    'osteoporosis', 'march', 'clinic', 'copyright', 'cancer', 'hsp', 'states', 'tumor', 'april',\n",
        "    'osteoblast', 'chemoresistance', 'nephropathy', 'hospital', 'moscow', 'analysis', 'aminelevulinic',\n",
        "    'biophysic', 'medication', 'staining', 'et', 'war', 'therapie', 'malignant', 'canine', 'ukraine',\n",
        "    'stroke', 'laboratory', 'graph', 'none', 'rights', 'caninemammary', 'reserved', 'tehran', 'radiotherapy',\n",
        "    'veterinary', 'diagnostic', 'ischaemic', 'grade', 'granule', 'microscope', 'tumour', 'united',\n",
        "    'overnight', 'group', 'histology', 'lysate', 'metastatic', 'dog', 'november', 'february', 'sclerose',\n",
        "    'december', 'breastcancer', 'psf', 'figure', 'reperfusion', 'craiova', 'methodology', 'tables',\n",
        "    'october', 'mammary', 'data', 'expression', 'diagnosis', 'lysis', 'doi', 'association', 'biopsy',\n",
        "    'denmark', 'research', 'dose', 'micropapillary', 'year', 'preprint', 'dept', 'histopathology', 'mellitus',\n",
        "    'phenotype', 'risk', 'sample', 'fibrosis', 'clinical', 'antibody', 'aortic', 'torino', 'biomarkers',\n",
        "    'tumorexpression', 'significant', 'manchester', 'genetic', 'stockholm', 'measurement', 'progression',\n",
        "    'patienten', 'survival', 'therapy', 'lungcancer', 'control', 'gfp', 'september', 'response', 'www', 'msc',\n",
        "    'invasive', 'fig', 'trial', 'iran', 'blotting', 'introduction', 'inflammation', 'gene', 'protein',\n",
        "    'slovenia', 'southampton', 'journal', 'treatment', 'freiburg', 'cell', 'cardiologist', 'pmcid', 'results',\n",
        "    'smoker', 'june', 'galway', 'tsklinikum', 'immunohisto', 'imaging', 'license', 'level', 'author',\n",
        "    'protocol', 'tissue', 'university', 'experiment', 'incubation', 'epilepsy', 'cirrhosis', 'july', 'volunteer',\n",
        "    'chemotherapy', 'breast', 'perra', 'ppix', 'may', 'immune', 'purification', 'mutation', 'mri', 'pmid',\n",
        "    'technique', 'histochemical', 'effect', 'psoriasis', 'vienna', 'lille', 'surgery', 'table', 'methods',\n",
        "    'http', 'target', 'associate', 'network', 'detection', 'dataset', 'performance', 'value', 'set', 'anti', 'accuracy',\n",
        "    'predict', 'pathway', 'signal', 'segmentation', 'detect', 'annotation', 'tool', 'therapeutic', 'information',\n",
        "    'demonstrate', 'dna', 'section', 'score', 'deep', 'identify', 'drug', 'negative', 'medical', 'background',\n",
        "    'induce', 'enhance', 'melanoma', 'medicine', 'promote', 'status', 'lung', 'performance',\n",
        "    'positive', 'perform', 'disclosure', 'ihc', 'cation', 'test', 'mitotic',\n",
        "    'stain', 'mouse', 'lesion', 'classification', 'learning', 'algorithm',\n",
        "    'digital', 'design', 'macrophage', 'classi', 'molecular', 'node',\n",
        "    'center', 'training', 'rate', 'tnbc', 'area', 'subtype', 'wang',\n",
        "    'specific', 'nuclear', 'application', 'evaluate', 'present', 'train',\n",
        "    'signi', 'role', 'lymph', 'radiomic', 'growth', 'mean', 'primary',\n",
        "    'nanoparticle', 'specimen', 'tme', 'marker', 'cohort', 'difference',\n",
        "    'inhibitor', 'assess', 'immunotherapy', 'change', 'tion', 'image', 'label', 'process',\n",
        "    'evolution', 'technology'\n",
        "}\n",
        "\n",
        "stop_words = stopwords_en.union(stopwords_de).union(extra_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e88008",
      "metadata": {
        "id": "51e88008"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_text_from_dataset(dataset_path):\n",
        "    df = pd.read_excel(dataset_path)\n",
        "    texts = df['abstract'].tolist()\n",
        "    processed_texts = []\n",
        "    for text in texts:\n",
        "        if not isinstance(text, str):\n",
        "            processed_texts.append(\"\")\n",
        "            continue\n",
        "        text = unicodedata.normalize('NFKC', text)\n",
        "        text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n",
        "        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n",
        "        text = re.sub(r'[^a-zA-Z0-9áéíóúÁÉÍÓÚüÜñÑ.,;:()\\-\\'\\\" ]+', ' ', text)\n",
        "        # Normaliza múltiples espacios a uno solo\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Recorta espacios al inicio y final\n",
        "        text = text.strip()\n",
        "        processed_texts.append(text)\n",
        "    return processed_texts\n",
        "\n",
        "def clean_text_remove_metadata(text):\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if re.search(r'\\S+@\\S+', line):  # Ignorar emails\n",
        "            continue\n",
        "        if re.search(r'http\\S+', line):  # Ignorar URLs\n",
        "            continue\n",
        "        if len(line) < 30:  # Ignorar líneas cortas\n",
        "            continue\n",
        "        if sum(1 for c in line if c.isupper()) > len(line) * 0.5:  # Ignorar líneas con muchas mayúsculas\n",
        "            continue\n",
        "        cleaned_lines.append(line)\n",
        "    return ' '.join(cleaned_lines)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_text_remove_metadata(text)\n",
        "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s]', ' ', text)  # Eliminar caracteres no alfabéticos\n",
        "    text = text.lower()  # Convertir a minúsculas\n",
        "    doc = nlp_en(text)\n",
        "\n",
        "    tokens = [token.lemma_ for token in doc if token.lemma_ not in stop_words and len(token.lemma_) > 2]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def train_lda(X):\n",
        "    lda = LatentDirichletAllocation(\n",
        "        n_components=n_topics,\n",
        "        max_iter=n_iter,\n",
        "        learning_decay=learning_decay,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    lda.fit(X)\n",
        "    return lda\n",
        "\n",
        "def vectorize_texts(texts):\n",
        "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, ngram_range=(1, 2))\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "def print_top_words(model, feature_names):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic #{topic_idx+1}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "        print()\n",
        "\n",
        "def get_dominant_topics(lda_model, X):\n",
        "    topic_distributions = lda_model.transform(X)\n",
        "    dominant_topics = topic_distributions.argmax(axis=1)\n",
        "    return dominant_topics\n",
        "\n",
        "def count_articles_per_topic(dominant_topics):\n",
        "    topic_counts = [0] * n_topics\n",
        "    for topic in dominant_topics:\n",
        "        topic_counts[topic] += 1\n",
        "    return topic_counts\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "def get_topic_distributions(lda_model, X):\n",
        "    topic_distributions = lda_model.transform(X)\n",
        "    return topic_distributions\n",
        "\n",
        "def analyze_topic_distributions(topic_distributions):\n",
        "    topic_counts = [0] * n_topics\n",
        "    for distribution in topic_distributions:\n",
        "        dominant_topic = distribution.argmax()\n",
        "        topic_counts[dominant_topic] += 1\n",
        "        print(f\"Distribución de temas: {distribution} -> Tema dominante: {dominant_topic+1}\")\n",
        "    return topic_counts\n",
        "\n",
        "def calculate_coherence(lda_model, vectorizer, X, n_top_words=10):\n",
        "    \"\"\"\n",
        "    Calcula la coherencia de los temas generados por el modelo LDA.\n",
        "    \"\"\"\n",
        "    # Obtener las palabras más representativas de cada tema\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "        topics.append(top_words)\n",
        "\n",
        "    # Calcular la coherencia entre las palabras top para cada tema\n",
        "    coherence_scores = []\n",
        "    for topic_words in topics:\n",
        "        coherence_score = 0\n",
        "        for i, word1 in enumerate(topic_words):\n",
        "            for word2 in topic_words[i+1:]:\n",
        "                # Buscar las co-ocurrencias de palabras en el corpus\n",
        "                word1_idx = vectorizer.vocabulary_.get(word1, -1)\n",
        "                word2_idx = vectorizer.vocabulary_.get(word2, -1)\n",
        "                if word1_idx != -1 and word2_idx != -1:\n",
        "                    # Calcular la probabilidad conjunta de estas palabras en el modelo\n",
        "                    prob_word1 = np.mean(X[:, word1_idx].toarray())\n",
        "                    prob_word2 = np.mean(X[:, word2_idx].toarray())\n",
        "                    coherence_score += prob_word1 * prob_word2  # Coherencia como probabilidad conjunta\n",
        "        coherence_scores.append(coherence_score)\n",
        "\n",
        "    # Promediar las coherencias de todos los temas\n",
        "    average_coherence = np.mean(coherence_scores)\n",
        "    print(f\"Coherencia promedio de los temas: {average_coherence}\")\n",
        "\n",
        "    return average_coherence\n",
        "\n",
        "def generate_title_with_gpt3(keywords, model=\"gpt-3.5-turbo\"):\n",
        "    # Construir el mensaje de la conversación\n",
        "    prompt = f\"Tomando en cuenta las siguientes palabras clave resúmelas en un título de pocas palabras que las aborden {', '.join(keywords)}\"\n",
        "    client = OpenAI(\n",
        "        api_key='apikey',\n",
        "    )\n",
        "    try:\n",
        "        # Hacer la solicitud a GPT-3\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,       # Puede ser \"gpt-3.5-turbo\" o \"gpt-4\" si lo tienes\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Eres un asistente que genera títulos concisos para temas de investigación.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=60,      # Limitar la longitud de la respuesta\n",
        "            n=1,                # Número de respuestas que queremos\n",
        "            temperature=0.7     # Controlar la creatividad en la respuesta (0.7 es un buen valor general)\n",
        "        )\n",
        "        # Extraer y devolver el título generado - FIX AQUÍ\n",
        "        title = response.choices[0].message.content.strip()\n",
        "        return title\n",
        "    except Exception as e:\n",
        "        print(f\"Error al generar título con GPT-3: {e}\")\n",
        "        # Fallback: crear título simple con las palabras clave\n",
        "        return f\"Tema sobre {', '.join(keywords[:3])}\"\n",
        "\n",
        "def generate_topic_titles_with_gpt3(lda_model, feature_names, n_top_words=10):\n",
        "    topic_titles = []\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]  # Obtener índices de las palabras más importantes\n",
        "        top_words = [feature_names[i] for i in top_words_indices]  # Obtener las palabras\n",
        "        # Usar GPT-3 para generar un título\n",
        "        title = generate_title_with_gpt3(top_words)\n",
        "        topic_titles.append(f\"Topic #{topic_idx + 1}: {title}\")\n",
        "    return topic_titles\n",
        "\n",
        "def assign_topics_to_documents(lda_model, X, topic_titles):\n",
        "    # Obtener las distribuciones de los temas para cada documento\n",
        "    topic_distributions = lda_model.transform(X)\n",
        "    dominant_topics = topic_distributions.argmax(axis=1)\n",
        "\n",
        "    # Asignar el tema dominante a cada documento\n",
        "    topics = [topic_titles[dominant_topic] for dominant_topic in dominant_topics]\n",
        "\n",
        "    return topics\n",
        "\n",
        "\n",
        "def process_group(texts, dataset_path):\n",
        "    print(f\"\\nProcesando grupo con {len(texts)} documentos.\")\n",
        "    processed_texts = [preprocess_text(text) for text in texts]\n",
        "    X, vectorizer = vectorize_texts(processed_texts)\n",
        "    lda_model = train_lda(X)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    print_top_words(lda_model, feature_names)\n",
        "\n",
        "    topic_distributions = get_topic_distributions(lda_model, X)\n",
        "    topic_counts = analyze_topic_distributions(topic_distributions)\n",
        "\n",
        "    for i in range(n_topics):\n",
        "        print(f\"Topic #{i+1} tiene {topic_counts[i]} artículos.\")\n",
        "\n",
        "    # Evaluar la coherencia de los temas\n",
        "    coherence_score = calculate_coherence(lda_model, vectorizer, X, n_top_words)\n",
        "\n",
        "    # Generar títulos para los temas usando GPT-3\n",
        "    topic_titles = generate_topic_titles_with_gpt3(lda_model, feature_names)\n",
        "\n",
        "    # Asignar los temas a los documentos y agregar al dataset\n",
        "    topics = assign_topics_to_documents(lda_model, X, topic_titles)\n",
        "\n",
        "    # Cargar el dataset original\n",
        "    df = pd.read_excel(dataset_path)\n",
        "\n",
        "    # Asignar la columna 'topic' al dataset\n",
        "    df['topic'] = topics\n",
        "\n",
        "    # Guardar el dataset actualizado\n",
        "    df.to_excel('dataset_with_topics.xlsx', index=False)\n",
        "\n",
        "    print(f\"Coherencia del modelo LDA: {coherence_score}\")\n",
        "\n",
        "    return lda_model, coherence_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e8ea8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33e8ea8f",
        "outputId": "296ebab4-ed86-4a4d-c2a5-773aeb4d34f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Procesando grupo con 110 documentos.\n",
            "Topic #1:\n",
            "use discuss human like mmp review mitochondrial high lipid remain biological acid transcription feature evs aim challenge include disease alone non outcome improve impact basal like basal base advance compare inflammatory diagnose extracellular knockout mammography understanding long combine well critical translational vivo molecule woman cadherin host discovery prostate interaction factor surface standard enable nucleic nucleic acid future delivery human blbc blbc cross determine increase long term matrix development term either death relate parameter part classical significantly field focus function due overall large wild wild type biology patient radiation characteristic strategy effective one dynamic efficacy utilize mrna reveal size lack approach cellular combination advantage order overview\n",
            "\n",
            "Topic #2:\n",
            "peptide oncolytic biosensor resistance material potential base gagnps electrochemical muc neural print strategy relevance oncolytic peptide use show limit promising develop prognosis relate need approach overcome promising approach polymer selective sensitive future use review development contact energy structure intrinsic among normal review notably analyze field line however noninvasive offer common reproduce remarkable regulatory acquire output cause component experimental prospective challenge efficacy solid accurate morphology affect furthermore within option construct exploration capacity characterization combine screening respectively reveal innovative task patient literature like artificial microenvironment characteristic versatility effective large advance edge prediction solution must shift map play activity early chemical concept death relevant contribute current\n",
            "\n",
            "Topic #3:\n",
            "datum patient base delivery emt different approach challenge prediction improve various allow review autophagy force microenvironment potential human provide object use assist time new predictive pipeline hold novel recent discuss cellular strategy domain system integration propose mechanism disease auroc accurately resistance mfs attention component neural device cause management prognosis evaluation feature early develop cmts combination agent include environmental impact artificial recasnet improvement across phase contrast precision highlight two svm understanding reduce extracellular distribution pathological semantic lead introduce despite describe quantitative contribution ultrasound multiple contribute devise current count efficientnet intelligence artificial intelligence furthermore integrate development address scale significantly feasibility nps overall omic\n",
            "\n",
            "Topic #4:\n",
            "nhg use brain high patient base prognostic low human wsis show potential prostate review ray deepgrade intermediate disease provide circulate system auc stratification whole grading two well case discuss assign peptide vessel datum histological feature novel scheme formation comprehensive inter increase generate nottingham early low intermediate non blood factor discover need characteristic testing include correlate typical morphological additionally hazard epigenetic expert decision clinicopathologic compare similar versus focus degradation angiogenesis assay significantly external review summarize understanding bacterial microvessel quality dynamic observer number development improve physical achieve uncover develop source low high indicate one non cancerous worldwide structure agreement mfs evaluation assessment however match understand ratio\n",
            "\n",
            "Topic #5:\n",
            "feature use high stemness meningioma type include patient volume intrinsic machine magnification extract simple datum different property checkpoint compare glioma especially within disease current metabolism development platform case three radiologist aid microwave dielectric property dielectric potential screen top feline approach mitosis total residual still correlate provide classifier visualization expert select highly evidence different type domain one validation density system scientific human recurrence auc base spectral early error murine vivo power robust achieve strategy surgical construct visualize observe work module across shift make variability new interest precision elucidate understanding theranostic white utilize scanner relate artificial however processing correlation summarise reliability cmts optimize provide evidence\n",
            "\n",
            "Distribución de temas: [0.01922066 0.01900682 0.92346295 0.01910091 0.01920866] -> Tema dominante: 3\n",
            "Distribución de temas: [0.01485147 0.01482508 0.94054303 0.0149024  0.01487802] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02587993 0.02582607 0.02641196 0.02616177 0.89572027] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02386207 0.02373313 0.0241341  0.02402455 0.90424615] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02232488 0.02219749 0.9101016  0.02262466 0.02275138] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02654142 0.02650681 0.8934009  0.02669412 0.02685675] -> Tema dominante: 3\n",
            "Distribución de temas: [0.04124269 0.04016124 0.04098139 0.83707301 0.04054167] -> Tema dominante: 4\n",
            "Distribución de temas: [0.87747488 0.03041876 0.0308973  0.03070249 0.03050657] -> Tema dominante: 1\n",
            "Distribución de temas: [0.8728125  0.03156975 0.0318013  0.03195672 0.03185973] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02922655 0.0287725  0.02924034 0.88376007 0.02900055] -> Tema dominante: 4\n",
            "Distribución de temas: [0.86496624 0.03345262 0.03387035 0.03382586 0.03388494] -> Tema dominante: 1\n",
            "Distribución de temas: [0.01768409 0.0176047  0.01763428 0.01765406 0.92942288] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02199098 0.02182781 0.02228229 0.91158236 0.02231656] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02132205 0.0211644  0.0215099  0.02162727 0.91437638] -> Tema dominante: 5\n",
            "Distribución de temas: [0.87902292 0.0299851  0.03013958 0.03056551 0.03028689] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02819071 0.02810042 0.02863394 0.88695812 0.02811681] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02060776 0.02049264 0.02072532 0.02060554 0.91756873] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02495981 0.02464159 0.02542186 0.89973749 0.02523926] -> Tema dominante: 4\n",
            "Distribución de temas: [0.90422264 0.02353376 0.024171   0.02428291 0.02378968] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02230187 0.90969383 0.02243256 0.02277878 0.02279296] -> Tema dominante: 2\n",
            "Distribución de temas: [0.02456822 0.02428963 0.02440195 0.02478308 0.90195713] -> Tema dominante: 5\n",
            "Distribución de temas: [0.03148242 0.03103606 0.03148505 0.03275606 0.87324041] -> Tema dominante: 5\n",
            "Distribución de temas: [0.0205822  0.02028593 0.02064464 0.91796981 0.02051742] -> Tema dominante: 4\n",
            "Distribución de temas: [0.9013641  0.02430588 0.02455556 0.02519481 0.02457964] -> Tema dominante: 1\n",
            "Distribución de temas: [0.91340861 0.02135992 0.02175299 0.02171954 0.02175896] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02531605 0.89887108 0.02559078 0.02512621 0.02509588] -> Tema dominante: 2\n",
            "Distribución de temas: [0.02172457 0.0212446  0.02156017 0.02161614 0.91385453] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02324923 0.02312875 0.02327411 0.90695907 0.02338884] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02683447 0.02646272 0.89332614 0.02679351 0.02658315] -> Tema dominante: 3\n",
            "Distribución de temas: [0.92212397 0.01941068 0.01954451 0.01947687 0.01944398] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02862039 0.88537848 0.02877437 0.02879543 0.02843133] -> Tema dominante: 2\n",
            "Distribución de temas: [0.03278964 0.03224137 0.032588   0.0323475  0.87003349] -> Tema dominante: 5\n",
            "Distribución de temas: [0.03104402 0.03056189 0.0307751  0.87682439 0.0307946 ] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02672881 0.02617525 0.89399404 0.02661186 0.02649003] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02725811 0.0265953  0.02698264 0.89250726 0.02665669] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02154594 0.02132277 0.91387031 0.02175508 0.02150591] -> Tema dominante: 3\n",
            "Distribución de temas: [0.91525425 0.0210789  0.02134881 0.02119199 0.02112605] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02459519 0.90238308 0.02470094 0.02414543 0.02417536] -> Tema dominante: 2\n",
            "Distribución de temas: [0.02532452 0.8984991  0.02541662 0.02528312 0.02547663] -> Tema dominante: 2\n",
            "Distribución de temas: [0.02490416 0.02397527 0.02443031 0.02401365 0.90267662] -> Tema dominante: 5\n",
            "Distribución de temas: [0.8985695  0.02513672 0.0255186  0.02532432 0.02545086] -> Tema dominante: 1\n",
            "Distribución de temas: [0.03065009 0.8768304  0.03103965 0.03073507 0.03074479] -> Tema dominante: 2\n",
            "Distribución de temas: [0.03037865 0.87876587 0.03047502 0.0303569  0.03002356] -> Tema dominante: 2\n",
            "Distribución de temas: [0.03195621 0.87271195 0.03207857 0.03181696 0.03143632] -> Tema dominante: 2\n",
            "Distribución de temas: [0.02534053 0.02497429 0.52822895 0.39628169 0.02517453] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02680617 0.0261922  0.02649884 0.89411889 0.0263839 ] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02575051 0.02519218 0.89788291 0.02576555 0.02540884] -> Tema dominante: 3\n",
            "Distribución de temas: [0.87821401 0.03020543 0.03036865 0.03074952 0.0304624 ] -> Tema dominante: 1\n",
            "Distribución de temas: [0.03487597 0.03428525 0.86163441 0.03477861 0.03442576] -> Tema dominante: 3\n",
            "Distribución de temas: [0.0208507  0.02053575 0.02074303 0.91729402 0.0205765 ] -> Tema dominante: 4\n",
            "Distribución de temas: [0.89903327 0.02513142 0.02536371 0.02519065 0.02528095] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02387635 0.02369734 0.02399866 0.90460151 0.02382613] -> Tema dominante: 4\n",
            "Distribución de temas: [0.03056899 0.03001623 0.03045169 0.03067683 0.87828626] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02102486 0.02068342 0.02114888 0.02101299 0.91612986] -> Tema dominante: 5\n",
            "Distribución de temas: [0.9069784  0.02305766 0.02321694 0.02348383 0.02326318] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02084495 0.02044094 0.9175276  0.02064495 0.02054155] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02487528 0.02474299 0.90073063 0.02484795 0.02480315] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02280846 0.02200535 0.02236742 0.02232396 0.91049481] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02185942 0.02158306 0.02167465 0.02199806 0.91288482] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02316941 0.02303982 0.02345837 0.02356518 0.90676722] -> Tema dominante: 5\n",
            "Distribución de temas: [0.0230711  0.022728   0.90737094 0.02370024 0.02312972] -> Tema dominante: 3\n",
            "Distribución de temas: [0.91711222 0.0207127  0.02081898 0.02073728 0.02061881] -> Tema dominante: 1\n",
            "Distribución de temas: [0.88851679 0.02760987 0.02796869 0.02795903 0.02794562] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02932787 0.02910009 0.02964114 0.02975153 0.88217937] -> Tema dominante: 5\n",
            "Distribución de temas: [0.01603046 0.93573375 0.01612391 0.01603142 0.01608045] -> Tema dominante: 2\n",
            "Distribución de temas: [0.0271809  0.02695468 0.02717225 0.89138086 0.02731131] -> Tema dominante: 4\n",
            "Distribución de temas: [0.028974   0.02866044 0.02891293 0.88459304 0.02885959] -> Tema dominante: 4\n",
            "Distribución de temas: [0.0323916  0.03112563 0.03131187 0.87371797 0.03145294] -> Tema dominante: 4\n",
            "Distribución de temas: [0.0273066  0.02712628 0.02731675 0.89089066 0.02735971] -> Tema dominante: 4\n",
            "Distribución de temas: [0.03264642 0.03292417 0.03249995 0.86951102 0.03241844] -> Tema dominante: 4\n",
            "Distribución de temas: [0.01766555 0.01759266 0.92912434 0.01781347 0.01780397] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02482236 0.02458569 0.02495644 0.90085068 0.02478484] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02830517 0.02807216 0.02867717 0.8868344  0.0281111 ] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02455923 0.02436263 0.02506734 0.02481964 0.90119117] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02595989 0.02560466 0.89589562 0.02613002 0.02640981] -> Tema dominante: 3\n",
            "Distribución de temas: [0.93944929 0.01509644 0.01511774 0.01517443 0.0151621 ] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02590995 0.02571791 0.026372   0.02614717 0.89585297] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02443904 0.02420623 0.02446768 0.90234647 0.02454058] -> Tema dominante: 4\n",
            "Distribución de temas: [0.90634798 0.02308425 0.02343531 0.02377547 0.02335699] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02877319 0.0282928  0.02867517 0.02920485 0.88505398] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02257755 0.02235351 0.90912669 0.02327882 0.02266343] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02375835 0.02336136 0.02349383 0.02366275 0.90572371] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02193352 0.02175261 0.02211899 0.91223259 0.02196229] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02114475 0.02090885 0.91552288 0.02122254 0.02120098] -> Tema dominante: 3\n",
            "Distribución de temas: [0.89531437 0.02588975 0.02616465 0.02650267 0.02612855] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02285166 0.02253727 0.02272534 0.90902913 0.02285659] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02797691 0.02757055 0.88888652 0.02777701 0.027789  ] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02315347 0.02308258 0.02328219 0.9071     0.02338175] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02013169 0.01972665 0.92018184 0.01998685 0.01997296] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02436704 0.02386525 0.02414907 0.90347541 0.02414323] -> Tema dominante: 4\n",
            "Distribución de temas: [0.89543102 0.02591723 0.02641462 0.02615945 0.02607768] -> Tema dominante: 1\n",
            "Distribución de temas: [0.91816339 0.02008279 0.02064359 0.02047241 0.02063782] -> Tema dominante: 1\n",
            "Distribución de temas: [0.01653407 0.01650002 0.01654498 0.016545   0.93387593] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02608809 0.02566397 0.89626336 0.02581432 0.02617027] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02304652 0.02275017 0.02327552 0.0230339  0.90789388] -> Tema dominante: 5\n",
            "Distribución de temas: [0.89114106 0.02708338 0.02725173 0.0273314  0.02719242] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02173068 0.02163724 0.91283694 0.02195594 0.0218392 ] -> Tema dominante: 3\n",
            "Distribución de temas: [0.02057014 0.02037538 0.02064162 0.91766745 0.02074541] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02777144 0.88904658 0.02770922 0.0277254  0.02774736] -> Tema dominante: 2\n",
            "Distribución de temas: [0.01794573 0.01792318 0.01798447 0.92814192 0.0180047 ] -> Tema dominante: 4\n",
            "Distribución de temas: [0.02524104 0.02500634 0.0254609  0.89895132 0.02534039] -> Tema dominante: 4\n",
            "Distribución de temas: [0.01768409 0.0176047  0.01763428 0.01765406 0.92942288] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02248191 0.02235352 0.90954078 0.02281236 0.02281143] -> Tema dominante: 3\n",
            "Distribución de temas: [0.01485147 0.01482508 0.94054303 0.0149024  0.01487802] -> Tema dominante: 3\n",
            "Distribución de temas: [0.01603046 0.93573375 0.01612391 0.01603142 0.01608045] -> Tema dominante: 2\n",
            "Distribución de temas: [0.01653407 0.01650002 0.01654498 0.016545   0.93387593] -> Tema dominante: 5\n",
            "Distribución de temas: [0.02766269 0.02755097 0.0278475  0.02787425 0.88906458] -> Tema dominante: 5\n",
            "Distribución de temas: [0.93944929 0.01509644 0.01511774 0.01517443 0.0151621 ] -> Tema dominante: 1\n",
            "Distribución de temas: [0.02446727 0.02425433 0.90238128 0.02438266 0.02451446] -> Tema dominante: 3\n",
            "Distribución de temas: [0.01794573 0.01792318 0.01798447 0.92814192 0.0180047 ] -> Tema dominante: 4\n",
            "Topic #1 tiene 22 artículos.\n",
            "Topic #2 tiene 11 artículos.\n",
            "Topic #3 tiene 24 artículos.\n",
            "Topic #4 tiene 28 artículos.\n",
            "Topic #5 tiene 25 artículos.\n",
            "Coherencia promedio de los temas: 0.58844807021258\n",
            "Coherencia del modelo LDA: 0.58844807021258\n",
            "Coherencia del modelo LDA: 0.58844807021258\n",
            "Dataset con temas asignados guardado como 'dataset_with_topics.xlsx'.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    dataset = 'dataset_con_documento.xlsx'  # Cambia aquí por tu ruta\n",
        "    raw_texts = load_text_from_dataset(dataset)\n",
        "    langs = [detect_language(text) for text in raw_texts]\n",
        "\n",
        "    texts_en = [text for text, lang in zip(raw_texts, langs) if lang == 'en']\n",
        "\n",
        "    lda_en, coherence_score = process_group(texts_en, dataset)\n",
        "    print(f\"Coherencia del modelo LDA: {coherence_score}\")\n",
        "    print(f\"Dataset con temas asignados guardado como 'dataset_with_topics.xlsx'.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
