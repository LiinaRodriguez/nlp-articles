{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc88fc4",
      "metadata": {
        "id": "edc88fc4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import unicodedata\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, get_scheduler, MarianMTModel, MarianTokenizer\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de95086",
      "metadata": {
        "id": "7de95086"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(str1, str2):\n",
        "    tokens1 = set(str1.lower().split())\n",
        "    tokens2 = set(str2.lower().split())\n",
        "    intersection = tokens1.intersection(tokens2)\n",
        "    union = tokens1.union(tokens2)\n",
        "    if not union:\n",
        "        return 0\n",
        "    return len(intersection) / len(union)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19330917",
      "metadata": {
        "id": "19330917"
      },
      "outputs": [],
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, texts, summaries, tokenizer, max_input_length=1024, max_summary_length=150):\n",
        "        self.texts = texts\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_summary_length = max_summary_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        input_enc = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_input_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        summary_enc = self.tokenizer(\n",
        "            summary,\n",
        "            max_length=self.max_summary_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        labels = summary_enc.input_ids.squeeze()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100  # Ignorar padding en loss\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_enc.input_ids.squeeze(),\n",
        "            'attention_mask': input_enc.attention_mask.squeeze(),\n",
        "            'labels': labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95610e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d95610e7",
        "outputId": "3073b8a4-7bc5-494e-83ac-e55b06f37ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando tokenizer y modelo...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargados 110 artículos y 110 abstracts.\n"
          ]
        }
      ],
      "source": [
        "# --------- CONFIGURACION ---------\n",
        "\n",
        "model_name = \"GanjinZero/biobart-base\"  # Cambia si tienes otro modelo bioBART\n",
        "\n",
        "# Inicializar tokenizer y modelo\n",
        "print(\"Cargando tokenizer y modelo...\")\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Función para limpiar texto\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Normaliza unicode (acentos, etc)\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    # Reemplaza saltos de línea, tabs por espacio\n",
        "    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n",
        "\n",
        "    # Elimina caracteres no imprimibles (control)\n",
        "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n",
        "\n",
        "    # Elimina caracteres raros excepto letras, números, signos básicos y espacios\n",
        "    text = re.sub(r'[^a-zA-Z0-9áéíóúÁÉÍÓÚüÜñÑ.,;:()\\-\\'\\\" ]+', ' ', text)\n",
        "\n",
        "    # Normaliza múltiples espacios a uno solo\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Recorta espacios al inicio y final\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# --- Carga del Excel y lectura de textos ---\n",
        "file_path = \"./dataset_con_documento.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Ruta base donde están los archivos txt\n",
        "base_path = \"./txt_convertidos\"\n",
        "\n",
        "texts = []\n",
        "summaries = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    file_name = row['documento']\n",
        "    file_path_txt = os.path.join(base_path, file_name)\n",
        "    with open(file_path_txt, 'r', encoding='utf-8') as f:\n",
        "        article_text = f.read()\n",
        "    texts.append(article_text)\n",
        "    summaries.append(row['abstract'])\n",
        "\n",
        "print(f\"Cargados {len(texts)} artículos y {len(summaries)} abstracts.\")\n",
        "\n",
        "# Aplicar limpieza a textos y summaries\n",
        "texts = [clean_text(t) for t in texts]\n",
        "summaries = [clean_text(s) for s in summaries]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ed929d",
      "metadata": {
        "id": "f5ed929d"
      },
      "outputs": [],
      "source": [
        "# Parámetros de entrenamiento\n",
        "batch_size = 8\n",
        "num_epochs = 12\n",
        "learning_rate = 4e-5\n",
        "max_input_length = 1024\n",
        "max_summary_length = 350"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe791d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffe791d6",
        "outputId": "a2bddcd0-7286-411f-e2a0-35427736dfb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño train: 74\n",
            "Tamaño val: 19\n",
            "Tamaño test: 17\n"
          ]
        }
      ],
      "source": [
        "# --- División en train, val y test ---\n",
        "\n",
        "# Separar test (15%)\n",
        "train_val_texts, test_texts, train_val_summaries, test_summaries = train_test_split(\n",
        "    texts, summaries, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "# Dividir train y val (20% del train_val para validación)\n",
        "train_texts, val_texts, train_summaries, val_summaries = train_test_split(\n",
        "    train_val_texts, train_val_summaries, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f'Tamaño train: {len(train_texts)}')\n",
        "print(f'Tamaño val: {len(val_texts)}')\n",
        "print(f'Tamaño test: {len(test_texts)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ae24b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ae24b1",
        "outputId": "d7fc6943-9245-46df-f230-8183e1633343"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Crear datasets\n",
        "train_dataset = SummarizationDataset(train_texts, train_summaries, tokenizer, max_input_length, max_summary_length)\n",
        "val_dataset = SummarizationDataset(val_texts, val_summaries, tokenizer, max_input_length, max_summary_length)\n",
        "test_dataset = SummarizationDataset(test_texts, test_summaries, tokenizer, max_input_length, max_summary_length)\n",
        "\n",
        "# Crear DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c31c82",
      "metadata": {
        "id": "49c31c82"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----- FUNCION PARA ENTRENAMIENTO -----\n",
        "def train_epoch(model, dataloader, optimizer, lr_scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# ----- FUNCION PARA EVALUACION -----\n",
        "def eval_epoch(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e57ac6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89e57ac6",
        "outputId": "a875ec24-abce-4dd7-a960-73fda6dd9e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 1.3395 | Validation Loss: 0.5990\n",
            "Epoch 2 | Train Loss: 0.4862 | Validation Loss: 0.4691\n",
            "Epoch 3 | Train Loss: 0.3306 | Validation Loss: 0.4473\n",
            "Epoch 4 | Train Loss: 0.2796 | Validation Loss: 0.4068\n",
            "Epoch 5 | Train Loss: 0.2727 | Validation Loss: 0.3799\n",
            "Epoch 6 | Train Loss: 0.2192 | Validation Loss: 0.3739\n",
            "Epoch 7 | Train Loss: 0.1961 | Validation Loss: 0.3725\n",
            "Epoch 8 | Train Loss: 0.2095 | Validation Loss: 0.3693\n",
            "Epoch 9 | Train Loss: 0.1741 | Validation Loss: 0.3676\n",
            "Epoch 10 | Train Loss: 0.1751 | Validation Loss: 0.3696\n",
            "Epoch 11 | Train Loss: 0.1634 | Validation Loss: 0.3690\n",
            "Epoch 12 | Train Loss: 0.2801 | Validation Loss: 0.3682\n"
          ]
        }
      ],
      "source": [
        "# --------- ENTRENAMIENTO ---------\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, device)\n",
        "    val_loss = eval_epoch(model, val_loader, device)\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c1c72e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c1c72e8",
        "outputId": "11653434-20fe-4479-9b93-fa4ace6e43ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artículo 1: Jaccard similarity = 0.1095\n",
            "Resumen generado: LIMCas (low- to intermediate-grade invasive mammary carcinomas with discohesive tumor cells with single-file infiltrative growth patterns dispersed in the fibrous stroma2. The differences between IDC and ILC, from clinicopathological features to prognostic outcomes, have been extensively reported in the literature, sometimes with conflicting results3 5. More recently, attention has turned to the molecular and evolutionary differ- ences between the two entities and their precursor lesions, laying the foundations for personalized management of breast cancer.\n",
            "Resumen original: This study describes lobular-like invasive mammary carcinomas (LLIMCas), a group of low- to intermediate-grade invasive mammary carcinomas with discohesive, diffusely infiltrative cells showing retained circumferential membranous immunoreactivity for both E-cadherin and p120. We analyzed the clinical-pathologic features of 166 LLIMCas compared to 104 classical invasive lobular carcinomas (ILCs) and 100 grade 1 and 2 invasive ductal carcinomas (IDCs). Tumor size and pT stage of LLIMCas were intermediate between IDCs and ILCs, and yet often underestimated on imaging and showed frequent positive margins on the first resection. Despite histomorphologic similarities to classical ILC, the discohesion in LLIMCa was independent of E-cadherin p120 immunophenotypic alteration. An exploratory, hypothesis-generating analysis of the genomic features of 14 randomly selected LLIMCas and classical ILCs (7 from each category) was performed utilizing an FDA-authorized targeted capture sequencing assay (MSK-IMPACT). None of the seven LLIMCas harbored CDH1 loss-of-function mutations, and none of the CDH1 alterations detected in two of the LLIMCas was pathogenic. In contrast, all seven ILCs harbored CDH1 loss-of-function mutations coupled with the loss of heterozygosity of the CDH1 wild-type allele. Four of the six evaluable LLIMCas were positive for CDH1 promoter methylation, which may partially explain the single-cell infiltrative morphology seen in LLIMCa. Further studies are warranted to better define the molecular basis of the discohesive cellular morphology in LLIMCa. Until more data becomes available, identifying LLIMCas and distinguishing them from typical IDCs and ILCs would be justified. In patients with LLIMCas, preoperative MRI should be entertained to guide surgical management.\n",
            "\n",
            "Artículo 2: Jaccard similarity = 0.1371\n",
            "Resumen generado: Mitochondria play critical roles in cellular homeostasis and have been implicated in numerous diseases, including cancer, neurodegenerative diseases, and cardiovascular disorders (Alston et al., 2017). Quantitative analysis of mitochondrial ultrastructure is essential for advancing our understanding of mitochondrial biology and the development of therapeutic strategies. Mitochondrial quantifications remain crucial to understanding mitochondrial content, and emerging techniques, including two-photon microscopy, allow for quantification of mitochondrial area (\n",
            "Resumen original: This review provides an overview of the current methods for quantifying mitochondrial ultrastructure, including cristae morphology, mitochondrial contact sites, and recycling machinery and a guide to utilizing electron microscopy to effectively measure these organelles. Quantitative analysis of mitochondrial ultrastructure is essential for understanding mitochondrial biology and developing therapeutic strategies for mitochondrial-related diseases. Techniques such as transmission electron microscopy (TEM) and serial block face-scanning electron microscopy, as well as how they can be combined with other techniques including confocal microscopy, super-resolution microscopy, and correlative light and electron microscopy are discussed. Beyond their limitations and challenges, we also offer specific magnifications that may be best suited for TEM analysis of mitochondrial, endoplasmic reticulum, and recycling machinery. Finally, perspectives on future quantification methods are offered.\n",
            "\n",
            "Artículo 3: Jaccard similarity = 0.2731\n",
            "Resumen generado: Mitosis is highly relevant to patient outcome assessment. This task is challenging for algorithms and human experts alike, with deterioration of algorithmic performance under shifts in image representations. Considerable covariate shifts occur when assessment is performed on different tumor types, images are acquired using different digitization devices, or specimens are produced in different laboratories. The challenge provided annotated histologic tumor images from six different domains and evaluated the algorithmic approaches for the Mitosis Domain Generalization Challenge Marc Aubreville a, , Nikolas Stathonikos b, Taryn A. Donovan c, Robert Klopfleisch d, Jonas Ammeling a, Jonathan Ganz a, Frauke Wilm e,f, Mitko Veta g, Samir Jabari h, Markus Eckstein i, Jonas Annuscheit j, Christian Krumnow j, Engin Bozaba k, Sercan ay r k, Hongyan Gu l, Xiang Anthony Chen l, Mostafa Jahanifar m, Adam Shephard m, Satoshi Kondo n, Satoshi Kasai o, Sujatha Kotte p, Maxime W.\n",
            "Resumen original: Recognition of mitotic figures in histologic tumor specimens is highly relevant to patient outcome assessment. This task is challenging for algorithms and human experts alike, with deterioration of algorithmic performance under shifts in image representations. Considerable covariate shifts occur when assessment is performed on different tumor types, images are acquired using different digitization devices, or specimens are produced in different laboratories. This observation motivated the inception of the 2022 challenge on MItosis Domain Generalization (MIDOG 2022). The challenge provided annotated histologic tumor images from six different domains and evaluated the algorithmic approaches for mitotic figure detection provided by nine challenge participants on ten independent domains. Ground truth for mitotic figure detection was established in two ways: a three-expert majority vote and an independent, immunohistochemistry-assisted set of labels. This work represents an overview of the challenge tasks, the algorithmic strategies employed by the participants, and potential factors contributing to their success. With an score of 0.764 for the top-performing team, we summarize that domain generalization across various tumor domains is possible with today s deep learning-based recognition pipelines. However, we also found that domain characteristics not present in the training set (feline as new species, spindle cell shape as new morphology and a new scanner) led to small but significant decreases in performance. When assessed against the immunohistochemistry-assisted reference standard, all methods resulted in reduced recall scores, with only minor changes in the order of participants in the ranking.\n",
            "\n",
            "Artículo 4: Jaccard similarity = 0.1204\n",
            "Resumen generado: Background Lymphovascular invasion (LVI), which refers to the presence of tumour emboli within the lymphatic and or vascular spaces in the peritumoural invasive area, is con- sidered as the initial and cornerstone step in the metastatic process. Despite the propensity of invasive BC cells to invade surrounding stroma, only those that can interact with endothelial cells and invade the vascular wall will develop LVI and complete metastatic spread 5, 6 . These findings are warranted to unravel the mechanistic role of CCNB1 in the development of LVI.\n",
            "Resumen original: Background Lymphovascular invasion (LVI) is regulated through complex molecular mechanisms. Cyclin B1 (CCNB1) was previously determined as being associated with LVI using large cohorts of breast cancer (BC) and artificial neural network (ANN) technique. In this study, we aimed to assess the association between CCNB1 and LVI, other clinicopathological and other LVI-related biomarkers at the molecular (RNA transcriptomic) and proteomic levels in BC. Methods Two transcriptomic BC cohorts (n 2834) were used to assess the association between the expression of CCNB1 at the mRNA level and clinicopathological characteristics and patient outcome. Tissue microarrays (TMAs) from a well-characterised BC cohort (n 2480) with long-term outcome were also used to assess the clinical significance of CCNB1 protein expression using immunohistochemistry. Results High CCNB1 mRNA expression was associated with aggressive tumour behaviour, including LVI, larger size, higher tumour grade, high lymph nodal stage, hormonal receptor negativity, HER2 positivity and poor clinical outcome (all p 0.0001). Similarly, high CCNB1 protein expression was associated with higher tumour grade, hormonal receptor negativity and HER2 positivity (all p 0.0001). Additionally, there was a significant association between CCNB1- and LVI-related biomarkers including N-cadherin, P-cadherin and TWIST2 at the transcriptomic and proteomic level. Multivariate analysis revealed that CCNB1 was an independent predictor of shorter BC-specific survival (HR 1.3; 95 CI 1.2 1.5; p 0.010). Conclusion CCNB1 is a key gene associated with LVI in BC and has prognostic value. More functional studies are warranted to unravel the mechanistic role of CCNB1 in the development of LVI.\n",
            "\n",
            "Artículo 5: Jaccard similarity = 0.3688\n",
            "Resumen generado: Objective This study developed an innovative biosensor strategy for the sensitive and selective detection of canine mammary tumor biomarkers, cancer antigen 15 3 (CA 15 3) and mucin 1 (MUC-1), integrating green silver nanoparticles (GAgNPs) with machine learning (ML) algorithms to achieve high diagnostic accuracy and potential for noninvasive early detection. The GAgNPs-enhanced electrochemical biosensor demonstrated selective and sensitive detection of CA 15 3 in serum and MUC-\n",
            "Resumen original: This study developed an innovative biosensor strategy for the sensitive and selective detection of canine mammary tumor biomarkers, cancer antigen 15 3 (CA 15 3) and mucin 1 (MUC-1), integrating green silver nanoparticles (GAgNPs) with machine learning (ML) algorithms to achieve high diagnostic accuracy and potential for noninvasive early detection. The GAgNPs-enhanced electrochemical biosensor demonstrated selective detection of CA 15 3 in serum and MUC-1 in tissue homogenates, with limits of detection (LODs) of 0.07 and 0.11 U mL 1, respectively. The nanoscale dimensions of the GAgNPs endowed them with electrochemically active surface areas, facilitating sensitive biomarker detection. Experimental studies targeted CA 15 3 and MUC-1 biomarkers in clinical samples, and the biosensor exhibited ease of use and good selectivity. Furthermore, ML algorithms were employed to analyze the electrochemical data and predict biomarker concentrations, enhancing the diagnostic accuracy. The Random Forest algorithm achieved 98 accuracy in tumor presence prediction, while an Artificial Neural Network attained 76 accuracy in CA 15 3-based tumor grade classification. The integration of ML techniques with the GAgNPs-based biosensor offers a promising approach for noninvasive, accurate, and early detection of canine mammary tumors, potentially revolutionizing veterinary diagnostics. This multilayered strategy, combining eco-friendly nanomaterials, electrochemical sensing, and ML algorithms, holds significant potential for advancing both biomedical research and clinical practice in the field of canine mammary tumor diagnostics.\n",
            "\n",
            "Artículo 6: Jaccard similarity = 0.1125\n",
            "Resumen generado: Background Breast cancer Deep learning Digital pathology Gene expression profiling Prosigna Received: 28 January 2024 Accepted: 26 February 2024 Published online: 9 April 2024 The Author(s) 2024 Clinical evaluation of deep learning-based risk profiling in breast cancer histopathology and comparison to an established multigene assay Yinxi Wang1,2 Wenwen Sun3,4 Emelie Karlsson3\n",
            "Resumen original: Purpose To evaluate the Stratipath Breast tool for image-based risk profiling and compare it with an established prognostic multigene assay for risk profiling in a real-world case series of estrogen receptor (ER)-positive and human epidermal growth factor receptor 2 (HER2)-negative early breast cancer patients categorized as intermediate risk based on classic clinicopathological variables and eligible for chemotherapy. Methods In a case series comprising 234 invasive ER-positive HER2-negative tumors, clinicopathological data including Prosigna results and corresponding HE-stained tissue slides were retrieved. The digitized HE slides were analysed by Stratipath Breast. Results Our findings showed that the Stratipath Breast analysis identified 49.6 of the clinically intermediate tumors as low risk and 50.4 as high risk. The Prosigna assay classified 32.5 , 47.0 and 20.5 tumors as low, intermediate and high risk, respectively. Among Prosigna intermediate-risk tumors, 47.3 were stratified as Stratipath low risk and 52.7 as high risk. In addition, 89.7 of Stratipath low-risk cases were classified as Prosigna low intermediate risk. The overall agreement between the two tests for low-risk and high-risk groups (N 124) was 71.0 , with a Cohen s kappa of 0.42. For both risk profiling tests, grade and Ki67 differed significantly between risk groups. Conclusion The results from this clinical evaluation of image-based risk stratification shows a considerable agreement to an established gene expression assay in routine breast pathology.\n",
            "\n",
            "Artículo 7: Jaccard similarity = 0.3770\n",
            "Resumen generado: Background Lung adenocarcinoma (LUAD), which accounts for a large proportion of lung cancers, is divided into five major subtypes based on histologic characteristics. The clinical characteristics, prognosis, and responses to treatments vary among subtypes. Here, we demonstrate that the variations of cell-cell contact energy result in the LUAD tissue architecture Modification of the cell contact energy changed the spheroid morphogenesis Zha et al., iScience 27, 109742 May 17, 2024 a 2024 The Author(s). Published by Elsevier Inc.\n",
            "Resumen original: Lung adenocarcinoma (LUAD), which accounts for a large proportion of lung cancers, is divided into five major subtypes based on histologic characteristics. The clinical characteristics, prognosis, and responses to treatments vary among subtypes. Here, we demonstrate that the variations of cell-cell contact energy result in the LUAD subtype-specific morphogenesis. We reproduced the morphologies of the papillary LUAD subtypes with the cellular Potts Model (CPM). Simulations and experimental validations revealed modifications of cell-cell contact energy changed the morphology from a papillary-like structure to micropapillary or solid subtype-like structures. Remarkably, differential gene expression analysis revealed subtype-specific expressions of genes relating to cell adhesion. Knockdown experiments of the micropapillary upregulated ITGA11 gene resulted in the morphological changes of the spheroids produced from an LUAD cell line PC9. This work shows the consequences of gene mutations and gene expressions on patient prognosis through differences in tissue composing physical forces among LUAD subtypes.\n",
            "\n",
            "Artículo 8: Jaccard similarity = 0.0942\n",
            "Resumen generado: Technology convergence integrates key technologies from various fields to address complex issues, acting as a major driver for future industrial and technological growth. This topic has attracted academics in technology management, policy-making, and competitive intelligence. As a catalyst for innovation, convergence fosters emerging and disruptive technologies. In innovation management, understanding convergence guides technology development and informs research investment. For science and tech nology policymakers, early detection allows adaptation of policy tools to match the current pace of industrial and technology development (Karvonen et al., 2010). Grasping the mechanisms underlying convergence is vital for its reliable prediction. Network analysis is increasingly recognized as a pivotal approach to understanding technology convergence, notably through the adoption of the technology network concept (\n",
            "Resumen original: Understanding the dynamics of technology convergence is indispensable for both academic and industrial perspectives. Traditional analyses have mainly focused on the link formation process, overlooking the role that persistence process plays in shaping technology networks. This paper endeavors to fill this gap by incorporating the persistence process into the analysis of technology convergence using the Separate Temporal Exponential Random Graph Model (STERGM). Utilizing a decade-long dataset of breast cancer drug patents, we provide a comprehensive view of technology convergence mechanisms and their predictive capabilities. Our findings reveal significant differences in network effects between formation and persistence processes, indicating that focusing on only one may misrepresent the evolution of technology networks. The combined model achieves an F1 score of 69.54 in empirical forecasting, confirming its practical utility. Additionally, we introduce Intensification Networks to examine how existing ties strengthen or weaken over time, uncovering the critical role of intensification in the long-term evolution of technology convergence. By capturing both the formation of new ties and the intensification of existing ones, our model offers a more nuanced and forward-looking understanding of convergence dynamics, particularly in identifying potential areas for future technology convergence.\n",
            "\n",
            "Artículo 9: Jaccard similarity = 0.6627\n",
            "Resumen generado: Background High-throughput sequencing Chromosome Conformation Capture (Hi- C) allows the study of DNA interactions and 3D chromosome folding at the genome- wide scale. Usually, these data are represented as matrices describing the binary contacts among the different chromosome regions. On the other hand, a graph-based representation can be advantageous to describe the complex topology achieved by the DNA in the nucleus of eukaryotic cells. Methods Here we discuss the use of a graph database for storing and analysing data achieved by performing Hi-C experiments. The main issue is the size of the produced data and, working with a graph approach, the consequent necessity of adequately managing a large number of edges (contacts) connecting nodes (genes), which represents the sources of information. For this, currently available graph visuali- sation tools and libraries fall short with Hi- C data. The use of graph databases, instead, supports both the analysis and the visualisation of the spatial pattern present in Hi-Cs data, in particular for comparing different experiments or for re-mapping omics data in a space-aware context efficiently. In particular, the possibility of describing graphs through statistical indicators and, even more, the capability of correlating them through statistical distributions allows highlighting similarities and differences among different Hi-CCC experiments, in different cell conditions or different cell types. Results These concepts have been implemented in NeoHiC, an open-source and user-friendly web application for the progressive visualisation and analysis of Hi-CT net- works based on the utilization of the Neo4j graph database (version 3.5). Conclusion With the accumulation of more experiments, the tool will provide invalu- able support to compare neighbours of genes across experiments and conditions, helping in highlighting changes in functional domains and identifying new co-organ- ised genomic compartments. Keywords: HiC, Chromatin capture, Graph databases, Graph visualisation Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n",
            "Resumen original: Background High-throughput sequencing Chromosome Conformation Capture (Hi-C) allows the study of DNA interactions and 3D chromosome folding at the genome-wide scale. Usually, these data are represented as matrices describing the binary contacts among the different chromosome regions. On the other hand, a graph-based representation can be advantageous to describe the complex topology achieved by the DNA in the nucleus of eukaryotic cells. Methods Here we discuss the use of a graph database for storing and analysing data achieved by performing Hi-C experiments. The main issue is the size of the produced data and, working with a graph-based representation, the consequent necessity of adequately managing a large number of edges (contacts) connecting nodes (genes), which represents the sources of information. For this, currently available graph visualisation tools and libraries fall short with Hi-C data. The use of graph databases, instead, supports both the analysis and the visualisation of the spatial pattern present in Hi-C data, in particular for comparing different experiments or for re-mapping omics data in a space-aware context efficiently. In particular, the possibility of describing graphs through statistical indicators and, even more, the capability of correlating them through statistical distributions allows highlighting similarities and differences among different Hi-C experiments, in different cell conditions or different cell types. Results These concepts have been implemented in NeoHiC, an open-source and user-friendly web application for the progressive visualisation and analysis of Hi-C networks based on the use of the Neo4j graph database (version 3.5). Conclusion With the accumulation of more experiments, the tool will provide invaluable support to compare neighbours of genes across experiments and conditions, helping in highlighting changes in functional domains and identifying new co-organised genomic compartments.\n",
            "\n",
            "Artículo 10: Jaccard similarity = 0.3933\n",
            "Resumen generado: Simple and complex carcinomas are the most common type of malignant Canine Mammary Tu mors (CMTs), with simple carcinomas exhibiting aggressive behavior and poorer prognostic. Stemness is an ability associated with cancer initiation, malignancy, and therapeutic resistance, but is still few elucidated in canine mammary tumor subtypes. Here, we first validated, using CMT samples, a previously published canine one-class logistic regression machine learning algorithm (OCLR) to predict stemness (mRNAsi) in canine cancer cells. Then, using the canine mRNA stemness index; NMF, non-negative matrix factorization; WES, Whole-exome sequencing; CSC, Cancer stem cells; CA, Correspondence analysis; MCA, Multiple correspondence analysis.\n",
            "Resumen original: Simple and complex carcinomas are the most common type of malignant Canine Mammary Tumors (CMTs), with simple carcinomas exhibiting aggressive behavior and poorer prognostic. Stemness is an ability associated with cancer initiation, malignancy, and therapeutic resistance, but is still few elucidated in canine mammary tumor subtypes. Here, we first validated, using CMT samples, a previously published canine one-class logistic regression machine learning algorithm (OCLR) to predict stemness (mRNAsi) in canine cancer cells. Then, using the canine mRNAsi, we observed that simple carcinomas exhibit higher stemness than complex carcinomas and other histological subtypes. Also, we confirmed that stemness is higher and associated with basal-like CMTs and with NMF2 metagene signature, a tumor-specific DNA-repair metagene signature. Using correlation analysis, we selected the top 50 genes correlated with higher stemness, and the top 50 genes correlated with lower stemness and further performed a gene set enrichment analysis to observe the biological processes enriched for these genes. Finally, we suggested two promise stemness-associated targets in CMTs, POLA2 and APEX1, especially in simple carcinomas. Thus, our work elucidates stemness as a potential mechanism behind the aggressiveness and development of canine mammary tumors, especially in simple carcinomas, describing evidence of a promising strategy to target this disease.\n",
            "\n",
            "Artículo 11: Jaccard similarity = 0.5427\n",
            "Resumen generado: Background Racial disparity in breast cancer after neoadjuvant chemotherapy (NAC) has worse distant recurrence-free survival (DRFS). Such racial disparity may be due to difference in density of portals for systemic cancer cell dissemination, called TMEM doorways, and pro-metastatic tumor microenvironment (TME). Here, we evaluate residual cancer specimens after NAC from 96 Black and 87 white women. TMEM doorsways are visualized by triple immunohistochemistry, and cancer stem cells by immunofluorescence for SOX9. The correlation between TMEM doorway score and other pro-Metastatic TME parameters with DRFS is examined using log-rank and multivariate Cox regression. Thus, we investigate whether Black, compared to white, patients are more likely to develop distant recur (49 vs 34.5 , p 0.07), receive mastectomy (69.8 vs 54 .\n",
            "Resumen original: Black, compared to white, women with residual estrogen receptor-positive (ER ) breast cancer after neoadjuvant chemotherapy (NAC) have worse distant recurrence-free survival (DRFS). Such racial disparity may be due to difference in density of portals for systemic cancer cell dissemination, called TMEM doorways, and pro-metastatic tumor microenvironment (TME). Here, we evaluate residual cancer specimens after NAC from 96 Black and 87 white women. TMEM doorways are visualized by triple immunohistochemistry, and cancer stem cells by immunofluorescence for SOX9. The correlation between TMEM doorway score and pro-metastatic TME parameters with DRFS is examined using log-rank and multivariate Cox regression. Black, compared to white, patients are more likely to develop distant recurrence (49 vs 34.5 , p 0.07), receive mastectomy (69.8 vs 54 , p 0.04), and have higher grade tumors (p 0.002). Tumors from Black patients have higher TMEM doorway and macrophages density overall (p 0.002; p 0.002, respectively) and in the ER HER2- (p 0.02; p 0.02, respectively), but not in the triple negative disease. Furthermore, high TMEM doorway score is associated with worse DRFS. TMEM doorway score is an independent prognostic factor in the entire study population (HR, 2.02; 95 CI, 1.18 3.46; p 0.01), with a strong trend in ER HER2- disease (HR, 2.38; 95 CI, 0.96 5.95; p 0.06). SOX9 expression is not associated with racial disparity in TME or outcome. In conclusion, higher TMEM doorway density in residual breast cancer after NAC is associated with higher distant recurrence risk, and Black patients are associated with higher TMEM doorway density, suggesting that TMEM doorway density may contribute to racial disparities in breast cancer.\n",
            "\n",
            "Artículo 12: Jaccard similarity = 0.5029\n",
            "Resumen generado: Background Changes in microcirculation of axillary lymph nodes (ALNs) may indicate metastasis. Reliable noninvasive imaging technique to quantify such variations is lacking. We aim to develop and investigate a contrast-free ultrasound quantitative microvasculature imaging technique for detection of metastatic ALN in vivo. Experimental design The proposed ultrasound-based technique, high-definition microvessel imaging (HDMI) provides superb images of tumor vasculature at sub-millimeter size scales and enables quantitative analysis of microvessels structures. We evaluated the new HDMI technique on 68 breast cancer patients with ultrasound- identified suspicious ipsilateral axillary nodes recommended for fine needle aspiration biopsy (FNAB). HDMI was conducted before the FNAB and vessel morphological features were extracted, analyzed, and the results were correlated with the histopathology.\n",
            "Resumen original: Purpose Changes in microcirculation of axillary lymph nodes (ALNs) may indicate metastasis. Reliable noninvasive imaging technique to quantify such variations is lacking. We aim to develop and investigate a contrast-free ultrasound quantitative microvasculature imaging technique for detection of metastatic ALN in vivo. Experimental design The proposed ultrasound-based technique, high-definition microvasculature imaging (HDMI) provides superb images of tumor microvasculature at sub-millimeter size scales and enables quantitative analysis of microvessels structures. We evaluated the new HDMI technique on 68 breast cancer patients with ultrasound-identified suspicious ipsilateral axillary lymph nodes recommended for fine needle aspiration biopsy (FNAB). HDMI was conducted before the FNAB and vessel morphological features were extracted, analyzed, and the results were correlated with the histopathology. Results Out of 15 evaluated quantitative HDMI biomarkers, 11 were significantly different in metastatic and reactive ALNs (10 with P 0.01 and one with 0.01 P 0.05). We further showed that through analysis of these biomarkers, a predictive model trained on HDMI biomarkers combined with clinical information (i.e., age, node size, cortical thickness, and BI-RADS score) could identify metastatic lymph nodes with an area under the curve of 0.9 (95 CI 0.82,0.98 ), sensitivity of 90 , and specificity of 88 . Conclusions The promising results of our morphometric analysis of HDMI on ALNs offer a new means of detecting lymph node metastasis when used as a complementary imaging tool to conventional ultrasound. The fact that it does not require injection of contrast agents simplifies its use in routine clinical practice.\n",
            "\n",
            "Artículo 13: Jaccard similarity = 0.0673\n",
            "Resumen generado: ast cancer detection has greatly improved over the past several decades, and increased emphasis has been placed on the mo- lecular and genetic features of breast cancer. Breast imaging modal- ities with high diagnostic accuracy have a signifi- cant impact on population health, as breast cancer has surpassed lung cancer as the most commonly diagnosed nonskin cancer globally and accounts for 1 in 8 cancer diagnoses overall.\n",
            "Resumen original: Objective: The rapid advancement of high-throughput technologies in the biomedical field has resulted in the accumulation of diverse omics data types, such as mRNA expression, DNA methylation, and microRNA expression, for studying various diseases. Integrating these multi-omics datasets enables a comprehensive understanding of the molecular basis of cancer and facilitates accurate prediction of disease progression. Methods: However, conventional approaches face challenges due to the dimensionality curse problem. This paper introduces a novel framework called Knowledge Distillation and Supervised Variational AutoEncoders utilizing View Correlation Discovery Network (KD-SVAE-VCDN) to address the integration of high-dimensional multi-omics data with limited common samples. Through our experimental evaluation, we demonstrate that the proposed KD-SVAE-VCDN architecture accurately predicts the progression of breast and kidney carcinoma by effectively classifying patients as long- or short-term survivors. Furthermore, our approach outperforms other state-of-the-art multi-omics integration models. Results: Our findings highlight the efficacy of the KD-SVAE-VCDN architecture in predicting the disease progression of breast and kidney carcinoma. By enabling the classification of patients based on survival outcomes, our model contributes to personalized and targeted treatments. The favorable performance of our approach in comparison to several existing models suggests its potential to contribute to the advancement of cancer understanding and management. Conclusion: The development of a robust predictive model capable of accurately forecasting disease progression at the time of diagnosis holds immense promise for advancing personalized medicine. By leveraging multi-omics data integration, our proposed KD-SVAE-VCDN framework offers an effective solution to this challenge, paving the way for more precise and tailored treatment strategies for patients with different types of cancer.\n",
            "\n",
            "Artículo 14: Jaccard similarity = 0.8020\n",
            "Resumen generado: Background Head and neck squamous cell carcinoma (HNSCC) is a heterogeneous tumor that is highly aggressive and ranks fifth among the most common cancers worldwide. Although, the researches that attempted to construct a diagnostic model were deficient in HNSCC. Currently, the gold standard for diagnosing head and neck tumors is pathology, but this requires a traumatic biopsy. There is still a lack of a noninvasive test for such a high incidence tumor. In order to screen genetic markers and construct diagnostic model, the methods of random forest (RF) and artificial neural network (ANN) were utilized. The data of H NSCC gene expression was accessed from Gene Expression Omnibus (GEO) database; we selected three datasets totally, and we combined 2 datasets (GSE6631 and GSE55547) for screening differentially expressed genes (DEGs) and chose another dataset (GSEA13399) for validation. Firstly, the 6 DEGs (CRISP3, SPINK5, KRT4, MMP1, MAL, SPP1) were screened by RF. Subsequently, ANN was applied to calculate the weights of 6 genes. Besides, we created a diagnosis model and nominated it as neuralHNSCCC, and the performance of neuralHNCC by area under curve (AUC) was verified using another dataset. Our model achieved an AUC of 0.998 in the training cohort, and 0.734 in the validation cohort. Furthermore, we used the Cell-type Identification using Estimating Relative Subsets of RNA Transcripts (CIBERSORT) algorithm to investigate the difference in immune cell infiltration between HNSC and normal tissues initially. The selected 6 DEG and the constructed novel diagnostic model of HNSCCC would make contributions to the diagnosis. Head and head squamous cancer carcinomas (HHSCC) mostly derive from the mucosal epithelium in the oral cavity, pharynx and larynx, and rank fifth in the world most common tumors. Every year, there has approxi- mately 540,000 new cases and estimated 108,500 deaths from HNS\n",
            "Resumen original: Head and neck squamous cell carcinoma (HNSCC) is a heterogeneous tumor that is highly aggressive and ranks fifth among the most common cancers worldwide. Although, the researches that attempted to construct a diagnostic model were deficient in HNSCC. Currently, the gold standard for diagnosing head and neck tumors is pathology, but this requires a traumatic biopsy. There is still a lack of a noninvasive test for such a high incidence tumor. In order to screen genetic markers and construct diagnostic model, the methods of random forest (RF) and artificial neural network (ANN) were utilized. The data of HNSCC gene expression was accessed from Gene Expression Omnibus (GEO) database; we selected three datasets totally, and we combined 2 datasets (GSE6631 and GSE55547) for screening differentially expressed genes (DEGs) and chose another dataset (GSE13399) for validation. Firstly, the 6 DEGs (CRISP3, SPINK5, KRT4, MMP1, MAL, SPP1) were screened by RF. Subsequently, ANN was applied to calculate the weights of 6 genes. Besides, we created a diagnostic model and nominated it as neuralHNSCC, and the performance of neuralHNSCC by area under curve (AUC) was verified using another dataset. Our model achieved an AUC of 0.998 in the training cohort, and 0.734 in the validation cohort. Furthermore, we used the Cell-type Identification using Estimating Relative Subsets of RNA Transcripts (CIBERSORT) algorithm to investigate the difference in immune cell infiltration between HNSCC and normal tissues initially. The selected 6 DEGs and the constructed novel diagnostic model of HNSCC would make contributions to the diagnosis.\n",
            "\n",
            "Artículo 15: Jaccard similarity = 0.9172\n",
            "Resumen generado: In India, prostate cancer (Prostate Cancer) is the second highest cause of death due to cancer in men globally. Proper detection and treatment are critical for halting or controlling the growth and spread of cancer cells within the human organism. However, evaluating these sorts of images is difficult and time-consuming, requiring histopathological image recognition as the most reliable method for treating PC because of its distinct visual characteristics. Risk eval- uation and treatment planning rely heavily on histological image-based Gleason grading of prostate tumors. This work introduces an innovative approach to Histological image analysis for prostate cancer diagnosis and grading. The Elephant Herding Optimization-based Hyper-parameter Convolutional Deep Belief Network (CDBN- EHO) is presented alongside a grading network head-optimized deep belief network technique for multi-task prediction. Leveraging an effective Bayesian inference method, fully linked Conditional Random Field (CRF) techniques are utilized for segmentation, with pairwise boundary capacities determined by a linear mixture of Gaussian kernels. The Multi-task approach aims to enhance performance by incorporating contextual informa- tion, leading to breakthrough results in the identification of epithelial cells and the grading of Gleason scores. The objective of this study is to demonstrate the effectiveness of the optimized deep beliefnetwork technique in improving diagnostic accuracy and efficiency for prostate Cancer diagnosis and Gleason staging in histological images.\n",
            "Resumen original: PC (Prostate Cancer) is the second highest cause of death due to cancer in men globally. Proper detection and treatment are critical for halting or controlling the growth and spread of cancer cells within the human organism. However, evaluating these sorts of images is difficult and time-consuming, requiring histopathological image recognition as the most reliable method for treating PC because of its distinct visual characteristics. Risk evaluation and treatment planning rely heavily on histological image-based Gleason grading of prostate tumors. This work introduces an innovative approach to histological image analysis for prostate cancer diagnosis and Gleason grading. The Elephant Herding Optimization-based Hyper-parameter Convolutional Deep Belief Network (CDBN-EHO) is presented alongside a grading network head-optimized deep belief network technique for multi-task prediction. Leveraging an effective Bayesian inference method, fully linked Conditional Random Field (CRF) techniques are utilized for segmentation, with pairwise boundary capacities determined by a linear mixture of Gaussian kernels. The multi-task approach aims to enhance performance by incorporating contextual information, leading to breakthrough results in the identification of epithelial cells and the grading of Gleason scores. The objective of this study is to demonstrate the effectiveness of the optimized deep belief network technique in improving diagnostic accuracy and efficiency for prostate cancer diagnosis and Gleason grading in histological images.\n",
            "\n",
            "Artículo 16: Jaccard similarity = 0.5798\n",
            "Resumen generado: Artificial intelligence (AI) on breast cancer (BC) diagnosis and management is expected to spread to the field of pathology in medicine. Integrating AI into routine pathology practice stands to improve diagnostic accuracy, thereby contributing to reducing avoidable errors. Additionally, AI has excelled in identifying invasive breast tumors and lymph node metastasis through its capacity to process large whole-slide images adeptly. Adaptive sampling techniques and powerful convolutional neural networks mark these achievements. The evaluation of hormonal status, which is imperative for BC treatment choices, has also been enhanced by AI quantitative analysis, aiding interobserver concordance and reliability. Breast cancer grading and mitotic count evaluation also benefit from AI intervention. AI-based frameworks effectively classify breast carcinomas, even for moderately graded cases that traditional methods struggle with. Moreover, AI-assisted mitotic figures quantification surpasses manual counting in precision and sensitivity, fostering improved prognosis.\n",
            "Resumen original: This review discusses the profound impact of artificial intelligence (AI) on breast cancer (BC) diagnosis and management within the field of pathology. It examines the various applications of AI across diverse aspects of BC pathology, highlighting key findings from multiple studies. Integrating AI into routine pathology practice stands to improve diagnostic accuracy, thereby contributing to reducing avoidable errors. Additionally, AI has excelled in identifying invasive breast tumors and lymph node metastasis through its capacity to process large whole-slide images adeptly. Adaptive sampling techniques and powerful convolutional neural networks mark these achievements. The evaluation of hormonal status, which is imperative for BC treatment choices, has also been enhanced by AI quantitative analysis, aiding interobserver concordance and reliability. Breast cancer grading and mitotic count evaluation also benefit from AI intervention. AI-based frameworks effectively classify breast carcinomas, even for moderately graded cases that traditional methods struggle with. Moreover, AI-assisted mitotic figures quantification surpasses manual counting in precision and sensitivity, fostering improved prognosis. The assessment of tumor-infiltrating lymphocytes in triple-negative breast cancer using AI yields insights into patient survival prognosis. Furthermore, AI-powered predictions of neoadjuvant chemotherapy response demonstrate potential for streamlining treatment strategies. Addressing limitations, such as preanalytical variables, annotation demands, and differentiation challenges, is pivotal for realizing AI s full potential in BC pathology. Despite the existing hurdles, AI s multifaceted contributions to BC pathology hold great promise, providing enhanced accuracy, efficiency, and standardization. Continued research and innovation are crucial for overcoming obstacles and fully harnessing AI s transformative capabilities in breast cancer diagnosis and assessment.\n",
            "\n",
            "Artículo 17: Jaccard similarity = 0.5000\n",
            "Resumen generado: The tumor microenvironment (TME) is integral to cancer progression, impacting metastasis and treatment response. It consists of diverse cell types, extracellular matrix components, and signaling molecules that interact to promote tumor growth and therapeutic resistance. Elucidating the intricate interactions between cancer cells and the TME is crucial in understanding cancer progression and therapeutic challenges. A critical process induced by TME signaling is the epithelial-mesenchymal transition (EMT), which enhances their motility and invasiveness and promote metastasis\n",
            "Resumen original: The tumor microenvironment (TME) is integral to cancer progression, impacting metastasis and treatment response. It consists of diverse cell types, extracellular matrix components, and signaling molecules that interact to promote tumor growth and therapeutic resistance. Elucidating the intricate interactions between cancer cells and the TME is crucial in understanding cancer progression and therapeutic challenges. A critical process induced by TME signaling is the epithelial-mesenchymal transition (EMT), wherein epithelial cells acquire mesenchymal traits, which enhance their motility and invasiveness and promote metastasis and cancer progression. By targeting various components of the TME, novel investigational strategies aim to disrupt the TME s contribution to the EMT, thereby improving treatment efficacy, addressing therapeutic resistance, and offering a nuanced approach to cancer therapy. This review scrutinizes the key players in the TME and the TME's contribution to the EMT, emphasizing avenues to therapeutically disrupt the interactions between the various TME components. Moreover, the article discusses the TME s implications for resistance mechanisms and highlights the current therapeutic strategies toward TME modulation along with potential caveats.\n",
            "\n",
            "Similitud Jaccard promedio en test: 0.3859\n"
          ]
        }
      ],
      "source": [
        "# --------- GENERACION DE RESUMEN ---------\n",
        "def generate_summary(text, model, tokenizer, device,\n",
        "                     max_length=max_input_length,\n",
        "                     num_beams=10,\n",
        "                     length_penalty=1.2,\n",
        "                     early_stopping=True):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_input_length, truncation=True).to(device)\n",
        "    summary_ids = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# --------- EVALUACION EN TEST ---------\n",
        "def evaluate_test_set(test_texts, test_summaries, model, tokenizer, device):\n",
        "    from collections import Counter\n",
        "\n",
        "    def jaccard_similarity(str1, str2):\n",
        "        tokens1 = set(str1.lower().split())\n",
        "        tokens2 = set(str2.lower().split())\n",
        "        intersection = tokens1.intersection(tokens2)\n",
        "        union = tokens1.union(tokens2)\n",
        "        if not union:\n",
        "            return 0\n",
        "        return len(intersection) / len(union)\n",
        "\n",
        "    model.eval()\n",
        "    similarities = []\n",
        "    for i, text in enumerate(test_texts):\n",
        "        gen_summary = generate_summary(text, model, tokenizer, device)\n",
        "        jaccard = jaccard_similarity(gen_summary, test_summaries[i])\n",
        "        similarities.append(jaccard)\n",
        "        print(f\"Artículo {i+1}: Jaccard similarity = {jaccard:.4f}\")\n",
        "        print(f\"Resumen generado: {gen_summary}\")\n",
        "        print(f\"Resumen original: {test_summaries[i]}\\n\")\n",
        "\n",
        "    avg_sim = sum(similarities) / len(similarities)\n",
        "    print(f\"Similitud Jaccard promedio en test: {avg_sim:.4f}\")\n",
        "\n",
        "\n",
        "evaluate_test_set(test_texts, test_summaries, model, tokenizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDbT5Mpv0nFu",
        "outputId": "86dc77e3-9915-49ae-85bd-c314361d3352"
      },
      "id": "uDbT5Mpv0nFu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30633f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b30633f5",
        "outputId": "a4271f80-086e-46f5-ffeb-62efc7af580a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores: {'rouge1': 0.497814861682714, 'rouge2': 0.42088737906258367, 'rougeL': 0.4650868964691566}\n"
          ]
        }
      ],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def evaluate_rouge(references, predictions):\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "    for ref, pred in zip(references, predictions):\n",
        "        score = scorer.score(ref, pred)\n",
        "        for key in scores:\n",
        "            scores[key].append(score[key].fmeasure)\n",
        "    avg_scores = {key: sum(vals)/len(vals) for key, vals in scores.items()}\n",
        "    return avg_scores\n",
        "\n",
        "# Ejemplo de uso con tus datos test\n",
        "predicted_summaries = [generate_summary(t, model, tokenizer, device) for t in test_texts]\n",
        "rouge_scores = evaluate_rouge(test_summaries, predicted_summaries)\n",
        "\n",
        "print(\"ROUGE Scores:\", rouge_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./model_finetuned_biobart\"\n",
        "\n",
        "# Guarda el modelo\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "# Guarda el tokenizer\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjzBfIaLLsQC",
        "outputId": "25329b83-de2c-47d4-a91b-7b3b7b7d7bab"
      },
      "id": "TjzBfIaLLsQC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_finetuned_biobart/tokenizer_config.json',\n",
              " './model_finetuned_biobart/special_tokens_map.json',\n",
              " './model_finetuned_biobart/vocab.json',\n",
              " './model_finetuned_biobart/merges.txt',\n",
              " './model_finetuned_biobart/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}